{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom nltk.stem import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\n\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom sklearn.metrics import f1_score\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":168,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train =  pd.read_csv('../input/climate-change-edsa2020-21/train.csv')\ntest = pd.read_csv('../input/climate-change-edsa2020-21/test.csv')","execution_count":169,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":170,"outputs":[{"output_type":"execute_result","execution_count":170,"data":{"text/plain":"   sentiment                                            message  tweetid\n0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n1          1  It's not like we lack evidence of anthropogeni...   126103\n2          2  RT @RawStory: Researchers say we have three ye...   698562\n3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n      <td>625221</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>It's not like we lack evidence of anthropogeni...</td>\n      <td>126103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>RT @RawStory: Researchers say we have three ye...</td>\n      <td>698562</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n      <td>573736</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n      <td>466954</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":171,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 15819 entries, 0 to 15818\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   sentiment  15819 non-null  int64 \n 1   message    15819 non-null  object\n 2   tweetid    15819 non-null  int64 \ndtypes: int64(2), object(1)\nmemory usage: 370.9+ KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment.value_counts()","execution_count":172,"outputs":[{"output_type":"execute_result","execution_count":172,"data":{"text/plain":" 1    8530\n 2    3640\n 0    2353\n-1    1296\nName: sentiment, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning\nRemoving the noise in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['message'] = train['message'].str.lower()\ntest['message'] = test['message'].str.lower()\n","execution_count":173,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer=PorterStemmer()\n\ndef remove_pattern(input_txt):\n    input_txt = re.sub(\"\\\\W\",\" \",input_txt)\n    input_txt = re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" \",input_txt)\n    \n    \n    # stem words\n    words = re.split(\"\\\\s+\",input_txt)\n    stemmed_words = [stemmer.stem(word=word) for word in words]\n    \n    return ' '.join(stemmed_words)","execution_count":174,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(input_txt):\n    text=re.sub(\"(\\\\W)\",\" \\\\1 \",input_txt)\n\n    # split based on whitespace\n    return re.split(\"\\\\s+\",input_txt)","execution_count":175,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":176,"outputs":[{"output_type":"execute_result","execution_count":176,"data":{"text/plain":"   sentiment                                            message  tweetid\n0          1  polyscimajor epa chief doesn't think carbon di...   625221\n1          1  it's not like we lack evidence of anthropogeni...   126103\n2          2  rt @rawstory: researchers say we have three ye...   698562\n3          1  #todayinmaker# wired : 2016 was a pivotal year...   573736\n4          1  rt @soynoviodetodas: it's 2016, and a racist, ...   466954","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>polyscimajor epa chief doesn't think carbon di...</td>\n      <td>625221</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>it's not like we lack evidence of anthropogeni...</td>\n      <td>126103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>rt @rawstory: researchers say we have three ye...</td>\n      <td>698562</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>#todayinmaker# wired : 2016 was a pivotal year...</td>\n      <td>573736</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>rt @soynoviodetodas: it's 2016, and a racist, ...</td>\n      <td>466954</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re \nstop = stopwords.words('english')\nremove_words = ['rt']\n\n# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\ntrain[\"message\"] = train[\"message\"].apply(lambda x: ' '.join([word for word in x.split() if word[0]!='#' and word[0]!='@'])) #Remove # and @\ntrain['message'] = train['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))            #Remove stopwords\ntrain['message'] = train['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (remove_words)]))    #Remove additional words\n\ntest[\"message\"] = test[\"message\"].apply(lambda x: ' '.join([word for word in x.split() if word[0]!='#' and word[0]!='@']))\ntest['message'] = test['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest['message'] = test['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (remove_words)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":177,"outputs":[{"output_type":"execute_result","execution_count":177,"data":{"text/plain":"   sentiment                                            message  tweetid\n0          1  polyscimajor epa chief doesn't think carbon di...   625221\n1          1  it's not like we lack evidence of anthropogeni...   126103\n2          2  rt @rawstory: researchers say we have three ye...   698562\n3          1  #todayinmaker# wired : 2016 was a pivotal year...   573736\n4          1  rt @soynoviodetodas: it's 2016, and a racist, ...   466954","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>polyscimajor epa chief doesn't think carbon di...</td>\n      <td>625221</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>it's not like we lack evidence of anthropogeni...</td>\n      <td>126103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>rt @rawstory: researchers say we have three ye...</td>\n      <td>698562</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>#todayinmaker# wired : 2016 was a pivotal year...</td>\n      <td>573736</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>rt @soynoviodetodas: it's 2016, and a racist, ...</td>\n      <td>466954</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Spliting the training data into X & Y variables\nWe split the training data into the X variables which will be the message(tweets that are tweeted out) and Y variables which will be the sentiment(If a person tweeting believes in Climate Change or not).\n\nThen we transform out data into numbers using CountVectorizer into a language that the computer can easily understand."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['sentiment']\nX = train['message']","execution_count":178,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1,2),tokenizer=tokenizer, min_df=2,max_df=0.70,analyzer='word',smooth_idf=False, preprocessor=remove_pattern ,stop_words=\"english\")\nX_cnt_vectorized = vectorizer.fit_transform(X)","execution_count":179,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n  'stop_words.' % sorted(inconsistent))\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Perfomance validation for train"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X_cnt_vectorized,y,test_size=0.08,shuffle=True, random_state=23)","execution_count":185,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the different train data models and F1 Score"},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, random_state=42)\nrfc.fit(X_train, y_train)\nrfc_cnt_pred = rfc.predict(X_test)","execution_count":186,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, rfc_cnt_pred, average=\"macro\")","execution_count":187,"outputs":[{"output_type":"execute_result","execution_count":187,"data":{"text/plain":"0.5667321767416644"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### SVM Linear Classifier Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_lin = svm.LinearSVC()\nsvm_lin.fit(X_train, y_train)\nsvm_cnt_pred = svm_lin.predict(X_test)","execution_count":188,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, svm_cnt_pred, average=\"macro\")","execution_count":189,"outputs":[{"output_type":"execute_result","execution_count":189,"data":{"text/plain":"0.6683787033069274"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### SGD Classifier Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nsgd_cnt_pred = sgd.predict(X_test)","execution_count":190,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, sgd_cnt_pred, average=\"macro\")","execution_count":191,"outputs":[{"output_type":"execute_result","execution_count":191,"data":{"text/plain":"0.6317591801921397"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Regression Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state = 23)\nlr.fit(X_train, y_train)\nlr_cnt_pred = lr.predict(X_test)","execution_count":192,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, lr_cnt_pred, average=\"macro\")","execution_count":193,"outputs":[{"output_type":"execute_result","execution_count":193,"data":{"text/plain":"0.6088427405216152"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Test Data"},{"metadata":{},"cell_type":"markdown","source":"### Spliting the training data into X & Y variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = test['message']\ntest_cnt_vect = vectorizer.transform(test_X)","execution_count":194,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing the test model data and F1 Score"},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cnt_pred = rfc.predict(test_cnt_vect)","execution_count":195,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM Linear Classifier Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cnt_pred = svm_lin.predict(test_cnt_vect)","execution_count":196,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SGD Classifier Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cnt_pred = sgd.predict(test_cnt_vect)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cnt_pred = lr.predict(test_cnt_vect)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Test predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['sentiment'] = y_cnt_pred","execution_count":197,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":198,"outputs":[{"output_type":"execute_result","execution_count":198,"data":{"text/plain":"                                             message  tweetid  sentiment\n0  europe will now be looking to china to make su...   169760          1\n1  combine this with the polling of staffers re c...    35326          1\n2  the scary, unimpeachable evidence that climate...   224985          1\n3  @karoli @morgfair @osborneink @dailykos \\nputi...   476263          1\n4  rt @fakewillmoore: 'female orgasms cause globa...   872928          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message</th>\n      <th>tweetid</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>europe will now be looking to china to make su...</td>\n      <td>169760</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>combine this with the polling of staffers re c...</td>\n      <td>35326</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the scary, unimpeachable evidence that climate...</td>\n      <td>224985</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@karoli @morgfair @osborneink @dailykos \\nputi...</td>\n      <td>476263</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rt @fakewillmoore: 'female orgasms cause globa...</td>\n      <td>872928</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Creating an output csv for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['tweetid','sentiment']].to_csv('testsubmission30.csv', index=False)","execution_count":161,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}